{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scats Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Date  Site_ID  Cycle_sequence    Combined_DateTime  \\\n",
      "67     2023-08-03     2368               1  2023-08-03 00:00:00   \n",
      "579    2023-08-03     2368               6  2023-08-03 00:05:00   \n",
      "1093   2023-08-03     2368              11  2023-08-03 00:10:00   \n",
      "1609   2023-08-03     2368              16  2023-08-03 00:15:00   \n",
      "2122   2023-08-03     2368              21  2023-08-03 00:20:00   \n",
      "...           ...      ...             ...                  ...   \n",
      "125877 2023-08-03     2368            1000  2023-08-03 23:35:00   \n",
      "126389 2023-08-03     2368            1005  2023-08-03 23:40:00   \n",
      "126897 2023-08-03     2368            1010  2023-08-03 23:45:00   \n",
      "127405 2023-08-03     2368            1015  2023-08-03 23:50:00   \n",
      "127919 2023-08-03     2368            1020  2023-08-03 23:55:00   \n",
      "\n",
      "        Planned_ cyclelength  Actual_ cyclelength  Required_ cyclelength  DS  \\\n",
      "67                        60                   60                     60  16   \n",
      "579                       60                   60                     60  27   \n",
      "1093                      60                   60                     60  29   \n",
      "1609                      60                   60                     60  28   \n",
      "2122                      60                   60                     60  28   \n",
      "...                      ...                  ...                    ...  ..   \n",
      "125877                    60                   60                     60  71   \n",
      "126389                    60                   60                     60  43   \n",
      "126897                    60                   60                     60  54   \n",
      "127405                    60                   60                     60  40   \n",
      "127919                    60                   60                     60  30   \n",
      "\n",
      "        Strategic_ approach_control_cy_time  Subsystem_No Stretched_ phase  \\\n",
      "67                                       20             6                A   \n",
      "579                                      20             6                A   \n",
      "1093                                     20             6                A   \n",
      "1609                                     21             6                A   \n",
      "2122                                     24             6                A   \n",
      "...                                     ...           ...              ...   \n",
      "125877                                   20             6                D   \n",
      "126389                                   22             6                D   \n",
      "126897                                   22             6                D   \n",
      "127405                                   20             6                D   \n",
      "127919                                   20             6                D   \n",
      "\n",
      "            A    B    C      D     E     F   G        Rounded_Time  \n",
      "67      178.0  1.0  1.0   17.0  19.0  14.0 NaN 2023-06-02 00:00:00  \n",
      "579     176.0  1.0  1.0   19.0  19.0  14.0 NaN 2023-06-02 00:05:00  \n",
      "1093    176.0  1.0  1.0   19.0  19.0  14.0 NaN 2023-06-02 00:10:00  \n",
      "1609    176.0  1.0  1.0   19.0  19.0  14.0 NaN 2023-06-02 00:15:00  \n",
      "2122    178.0  1.0  1.0   17.0  19.0  14.0 NaN 2023-06-02 00:20:00  \n",
      "...       ...  ...  ...    ...   ...   ...  ..                 ...  \n",
      "125877   22.0  1.0  1.0  173.0  19.0  14.0 NaN 2023-06-02 23:35:00  \n",
      "126389   22.0  1.0  1.0  173.0  19.0  14.0 NaN 2023-06-02 23:40:00  \n",
      "126897   22.0  1.0  1.0  173.0  19.0  14.0 NaN 2023-06-02 23:45:00  \n",
      "127405   22.0  1.0  1.0  173.0  19.0  14.0 NaN 2023-06-02 23:50:00  \n",
      "127919   22.0  1.0  1.0  173.0  19.0  14.0 NaN 2023-06-02 23:55:00  \n",
      "\n",
      "[288 rows x 19 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nsdsc0\\AppData\\Local\\Temp\\ipykernel_65928\\3665205939.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Signalfilt_df.drop_duplicates(subset='Combined_DateTime', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Read the 2 CSV files as dataframe and list the name of the columns\n",
    "Signaldf = pd.read_csv('C:/Users/nsdsc0/OneDrive - WSP O365/DATA_ANALYTICS/Python/SCATS/WEST_20230308.SM.csv')\n",
    "\n",
    "Signaldf\n",
    "  \n",
    "# Convert 'Cycle_starttime' and 'Date' columns to datetime format\n",
    "\n",
    "Signaldf['Cycle_ starttime'] = pd.to_datetime(Signaldf['Cycle_ starttime'])\n",
    "\n",
    "Signaldf['Date'] = pd.to_datetime(Signaldf['Date'])\n",
    "\n",
    "# Round the 'Cycle_starttime' column to the nearest 5-minute interval\n",
    "Signaldf['Rounded_Time'] = Signaldf['Cycle_ starttime'].dt.floor('5T')\n",
    "\n",
    "# Create a new column by combining the 'Date' column and the rounded time\n",
    "#Signaldf['Combined_DateTime'] = pd.to_datetime(Signaldf['Date']) + Signaldf['Rounded_Time'].dt.time\n",
    "\n",
    "# Combine 'Date' and 'Rounded_Time' into a new column\n",
    "Signaldf['Combined_DateTime'] = Signaldf['Date'].astype(str) + ' ' + Signaldf['Rounded_Time'].dt.time.astype(str)\n",
    "\n",
    "# Round down the 'Combined_DateTime' column to the nearest 5 minutes\n",
    "#Signaldf['Combined_DateTime'] = Signaldf['Combined_DateTime'].dt.floor('5min')\n",
    "\n",
    "# Rename the 'Rounded_Combined_DateTime' column to 'Combined_DateTime'\n",
    "Signaldf = Signaldf.rename(columns={'Rounded_Combined_DateTime': 'Combined_DateTime'})\n",
    "\n",
    "# Move the 'Combined_DateTime' column to be the 5th column\n",
    "cols = list(Signaldf.columns)\n",
    "cols.insert(4, cols.pop(cols.index('Combined_DateTime')))\n",
    "Signaldf = Signaldf[cols]\n",
    "\n",
    "#Filter the rows in the 'Signaldf' DataFrame where the 'Site_ID' column that has the value \"2368\"\n",
    "Signalfilt_df = Signaldf[Signaldf['Site_ID'] == 2368]\n",
    "\n",
    "# Remove duplicate values in the 'Combined_DateTime' column\n",
    "Signalfilt_df.drop_duplicates(subset='Combined_DateTime', inplace=True)\n",
    "\n",
    "# Drop Cycle_starttime column\n",
    "Signalfilt_df = Signalfilt_df.drop('Cycle_ starttime', axis=1)\n",
    "\n",
    "# Print the updated DataFrame\n",
    "print(Signalfilt_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Date  Site_ID  Error   Combined_DateTime  2368-1  2368-10  2368-11  \\\n",
      "0     8/03/2023     2368  False 2023-08-03 00:00:00       4        7       23   \n",
      "24    8/03/2023     2368  False 2023-08-03 00:05:00      13        4       17   \n",
      "48    8/03/2023     2368  False 2023-08-03 00:10:00      19        8       12   \n",
      "72    8/03/2023     2368  False 2023-08-03 00:15:00       4        3       22   \n",
      "96    8/03/2023     2368  False 2023-08-03 00:20:00       6        4       10   \n",
      "...         ...      ...    ...                 ...     ...      ...      ...   \n",
      "6792  8/03/2023     2368  False 2023-08-03 23:35:00       6        3       16   \n",
      "6816  8/03/2023     2368  False 2023-08-03 23:40:00       7        4        8   \n",
      "6840  8/03/2023     2368  False 2023-08-03 23:45:00       5        4       13   \n",
      "6864  8/03/2023     2368  False 2023-08-03 23:50:00       4        0        8   \n",
      "6888  8/03/2023     2368  False 2023-08-03 23:55:00       3        3        4   \n",
      "\n",
      "      2368-12  2368-13  2368-14  ...  2368-22  2368-23  2368-24  2368-3  \\\n",
      "0          27        1       12  ...       85        2        1      11   \n",
      "24         16        1        7  ...       32        4        0       8   \n",
      "48         21        3        5  ...       35        1        0      12   \n",
      "72          7        0        7  ...       33        5        1       9   \n",
      "96         12        1        5  ...       29       10        1       8   \n",
      "...       ...      ...      ...  ...      ...      ...      ...     ...   \n",
      "6792        8        8       22  ...       37        2        1       6   \n",
      "6816        9        8       19  ...       50        2        4       4   \n",
      "6840        7        2       13  ...       64        0        2       1   \n",
      "6864        8        6       12  ...       50        0        0       5   \n",
      "6888        9        4       20  ...       72        1        0       5   \n",
      "\n",
      "      2368-4  2368-5  2368-6  2368-7  2368-8  2368-9  \n",
      "0         16       0      20     120      23       9  \n",
      "24        13       0       6      30      17      13  \n",
      "48         8       0       4      28      20       8  \n",
      "72        15       0       3      24      23      10  \n",
      "96        11       0       3      25      18       6  \n",
      "...      ...     ...     ...     ...     ...     ...  \n",
      "6792      13       0      18      38      27       8  \n",
      "6816      12       0      23      62      10      10  \n",
      "6840      10       0       3      86      15       6  \n",
      "6864      11       0       6      69      13      14  \n",
      "6888      11       0       1      75      21      11  \n",
      "\n",
      "[288 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "TraffCountCHKdf = pd.read_csv('C:/Users/nsdsc0/OneDrive - WSP O365/DATA_ANALYTICS/Python/SCATS/WEST_20230308.VS.csv')\n",
    "TraffCountCHKdf\n",
    "\n",
    "\n",
    "# Convert 'Start_time' column to datetime format\n",
    "TraffCountCHKdf['Start_time'] = pd.to_datetime(TraffCountCHKdf['Start_time'])\n",
    "\n",
    "# Create a new column 'Combined_DateTime' by combining 'Date' and 'Start_time'\n",
    "TraffCountCHKdf['Combined_DateTime'] = TraffCountCHKdf['Date'].astype(str) + ' ' + TraffCountCHKdf['Start_time'].dt.time.astype(str)\n",
    "\n",
    "# Convert 'Combined_DateTime' column to datetime format\n",
    "TraffCountCHKdf['Combined_DateTime'] = pd.to_datetime(TraffCountCHKdf['Combined_DateTime'])\n",
    "\n",
    "# Round 'Combined_DateTime' column down to the nearest 5 minutes\n",
    "TraffCountCHKdf['Rounded_Combined_DateTime'] = TraffCountCHKdf['Combined_DateTime'].dt.floor('5min')\n",
    "\n",
    "#Filter the rows in the 'TraffCount3df' DataFrame where the 'Site_ID' column that has the value \"2368\"\n",
    "filtered_df = TraffCountCHKdf[TraffCountCHKdf['Site_ID'] == 2368]\n",
    "\n",
    "# Pivot the DataFrame to create separate columns for each unique Detector_ID\n",
    "pivoted_df = filtered_df.pivot(index=['Combined_DateTime', 'Site_ID'], columns='Detector_ID', values='Traffic_Volume')\n",
    "\n",
    "# Reset the index to make the combined DateTime and Site_ID columns regular columns\n",
    "pivoted_df = pivoted_df.reset_index()\n",
    "\n",
    "# Merge the pivoted DataFrame back into the original DataFrame based on the combined DateTime and Site_ID columns\n",
    "merged_df = filtered_df.merge(pivoted_df, on=['Combined_DateTime', 'Site_ID'], how='left')\n",
    "\n",
    "# Remove the 'Combined_DateTime' column\n",
    "merged_df.drop('Combined_DateTime', axis=1, inplace=True)\n",
    "\n",
    "# Rename the 'Rounded_Combined_DateTime' column to 'Combined_DateTime'\n",
    "merged_df.rename(columns={'Rounded_Combined_DateTime': 'Combined_DateTime'}, inplace=True)\n",
    "\n",
    "# Drop the 'Detector_ID' column\n",
    "merged_df.drop('Detector_ID', axis=1, inplace=True)\n",
    "\n",
    "# Remove duplicate values in the 'Combined_DateTime' column\n",
    "merged_df.drop_duplicates(subset='Combined_DateTime', inplace=True)\n",
    "\n",
    "# Drop Cycle_starttime column\n",
    "merged_df = merged_df.drop(['Start_time', 'Traffic_Volume'], axis=1)\n",
    "\n",
    "# Print the updated DataFrame\n",
    "print(merged_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in Signalfilt_df: 288\n",
      "Number of rows in merged_df: 288\n"
     ]
    }
   ],
   "source": [
    "\n",
    "row_count = Signalfilt_df.shape[0]\n",
    "print(\"Number of rows in Signalfilt_df:\", row_count)\n",
    "row_count = merged_df.shape[0]\n",
    "print(\"Number of rows in merged_df:\", row_count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You are trying to merge on object and datetime64[ns] columns. If you wish to proceed you should use pd.concat",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\nsdsc0\\OneDrive - WSP O365\\DATA_ANALYTICS\\Python\\SCATS\\ScatsCHK.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/nsdsc0/OneDrive%20-%20WSP%20O365/DATA_ANALYTICS/Python/SCATS/ScatsCHK.ipynb#X34sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m Newmerged2_df \u001b[39m=\u001b[39m Signalfilt_df\u001b[39m.\u001b[39;49mmerge(merged_df, on\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mCombined_DateTime\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nsdsc0/OneDrive%20-%20WSP%20O365/DATA_ANALYTICS/Python/SCATS/ScatsCHK.ipynb#X34sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(Newmerged2_df)\n",
      "File \u001b[1;32mc:\\Users\\nsdsc0\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:9351\u001b[0m, in \u001b[0;36mDataFrame.merge\u001b[1;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m   9332\u001b[0m \u001b[39m@Substitution\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   9333\u001b[0m \u001b[39m@Appender\u001b[39m(_merge_doc, indents\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m   9334\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmerge\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   9347\u001b[0m     validate: \u001b[39mstr\u001b[39m \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   9348\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame:\n\u001b[0;32m   9349\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mreshape\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmerge\u001b[39;00m \u001b[39mimport\u001b[39;00m merge\n\u001b[1;32m-> 9351\u001b[0m     \u001b[39mreturn\u001b[39;00m merge(\n\u001b[0;32m   9352\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[0;32m   9353\u001b[0m         right,\n\u001b[0;32m   9354\u001b[0m         how\u001b[39m=\u001b[39;49mhow,\n\u001b[0;32m   9355\u001b[0m         on\u001b[39m=\u001b[39;49mon,\n\u001b[0;32m   9356\u001b[0m         left_on\u001b[39m=\u001b[39;49mleft_on,\n\u001b[0;32m   9357\u001b[0m         right_on\u001b[39m=\u001b[39;49mright_on,\n\u001b[0;32m   9358\u001b[0m         left_index\u001b[39m=\u001b[39;49mleft_index,\n\u001b[0;32m   9359\u001b[0m         right_index\u001b[39m=\u001b[39;49mright_index,\n\u001b[0;32m   9360\u001b[0m         sort\u001b[39m=\u001b[39;49msort,\n\u001b[0;32m   9361\u001b[0m         suffixes\u001b[39m=\u001b[39;49msuffixes,\n\u001b[0;32m   9362\u001b[0m         copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m   9363\u001b[0m         indicator\u001b[39m=\u001b[39;49mindicator,\n\u001b[0;32m   9364\u001b[0m         validate\u001b[39m=\u001b[39;49mvalidate,\n\u001b[0;32m   9365\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\nsdsc0\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:107\u001b[0m, in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[39m@Substitution\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mleft : DataFrame or named Series\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     91\u001b[0m \u001b[39m@Appender\u001b[39m(_merge_doc, indents\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m     92\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmerge\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    105\u001b[0m     validate: \u001b[39mstr\u001b[39m \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    106\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame:\n\u001b[1;32m--> 107\u001b[0m     op \u001b[39m=\u001b[39m _MergeOperation(\n\u001b[0;32m    108\u001b[0m         left,\n\u001b[0;32m    109\u001b[0m         right,\n\u001b[0;32m    110\u001b[0m         how\u001b[39m=\u001b[39;49mhow,\n\u001b[0;32m    111\u001b[0m         on\u001b[39m=\u001b[39;49mon,\n\u001b[0;32m    112\u001b[0m         left_on\u001b[39m=\u001b[39;49mleft_on,\n\u001b[0;32m    113\u001b[0m         right_on\u001b[39m=\u001b[39;49mright_on,\n\u001b[0;32m    114\u001b[0m         left_index\u001b[39m=\u001b[39;49mleft_index,\n\u001b[0;32m    115\u001b[0m         right_index\u001b[39m=\u001b[39;49mright_index,\n\u001b[0;32m    116\u001b[0m         sort\u001b[39m=\u001b[39;49msort,\n\u001b[0;32m    117\u001b[0m         suffixes\u001b[39m=\u001b[39;49msuffixes,\n\u001b[0;32m    118\u001b[0m         copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m    119\u001b[0m         indicator\u001b[39m=\u001b[39;49mindicator,\n\u001b[0;32m    120\u001b[0m         validate\u001b[39m=\u001b[39;49mvalidate,\n\u001b[0;32m    121\u001b[0m     )\n\u001b[0;32m    122\u001b[0m     \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39mget_result()\n",
      "File \u001b[1;32mc:\\Users\\nsdsc0\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:704\u001b[0m, in \u001b[0;36m_MergeOperation.__init__\u001b[1;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    696\u001b[0m (\n\u001b[0;32m    697\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mleft_join_keys,\n\u001b[0;32m    698\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mright_join_keys,\n\u001b[0;32m    699\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjoin_names,\n\u001b[0;32m    700\u001b[0m ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_merge_keys()\n\u001b[0;32m    702\u001b[0m \u001b[39m# validate the merge keys dtypes. We may need to coerce\u001b[39;00m\n\u001b[0;32m    703\u001b[0m \u001b[39m# to avoid incompatible dtypes\u001b[39;00m\n\u001b[1;32m--> 704\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_coerce_merge_keys()\n\u001b[0;32m    706\u001b[0m \u001b[39m# If argument passed to validate,\u001b[39;00m\n\u001b[0;32m    707\u001b[0m \u001b[39m# check if columns specified as unique\u001b[39;00m\n\u001b[0;32m    708\u001b[0m \u001b[39m# are in fact unique.\u001b[39;00m\n\u001b[0;32m    709\u001b[0m \u001b[39mif\u001b[39;00m validate \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\nsdsc0\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:1267\u001b[0m, in \u001b[0;36m_MergeOperation._maybe_coerce_merge_keys\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1265\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1266\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m needs_i8_conversion(lk\u001b[39m.\u001b[39mdtype) \u001b[39mand\u001b[39;00m needs_i8_conversion(rk\u001b[39m.\u001b[39mdtype):\n\u001b[1;32m-> 1267\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1268\u001b[0m \u001b[39melif\u001b[39;00m is_datetime64tz_dtype(lk\u001b[39m.\u001b[39mdtype) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_datetime64tz_dtype(\n\u001b[0;32m   1269\u001b[0m     rk\u001b[39m.\u001b[39mdtype\n\u001b[0;32m   1270\u001b[0m ):\n\u001b[0;32m   1271\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n",
      "\u001b[1;31mValueError\u001b[0m: You are trying to merge on object and datetime64[ns] columns. If you wish to proceed you should use pd.concat"
     ]
    }
   ],
   "source": [
    "Newmerged2_df = Signalfilt_df.merge(merged_df, on='Combined_DateTime')\n",
    "print(Newmerged2_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOR YOUR CHECK OLD CODES as I was building up the top cell for TraffCountCHKdf\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "TraffCountCHKdf = pd.read_csv('....../WEST_20230308.VS.csv')\n",
    "\n",
    "# Convert 'Start_time' column to datetime format\n",
    "TraffCountCHKdf['Start_time'] = pd.to_datetime(TraffCountCHKdf['Start_time'])\n",
    "\n",
    "# Create a new column 'Combined_DateTime' by combining 'Date' and 'Start_time'\n",
    "TraffCountCHKdf['Combined_DateTime'] = TraffCountCHKdf['Date'].astype(str) + ' ' + TraffCountCHKdf['Start_time'].dt.time.astype(str)\n",
    "\n",
    "# Convert 'Combined_DateTime' column to datetime format\n",
    "TraffCountCHKdf['Combined_DateTime'] = pd.to_datetime(TraffCountCHKdf['Combined_DateTime'])\n",
    "\n",
    "# Round 'Combined_DateTime' column down to the nearest 5 minutes\n",
    "TraffCountCHKdf['Rounded_Combined_DateTime'] = TraffCountCHKdf['Combined_DateTime'].dt.floor('5min')\n",
    "\n",
    "# Filter the rows in the 'TraffCountCHKdf' DataFrame where the 'Site_ID' column has the value \"2368\"\n",
    "filtered_df = TraffCountCHKdf[TraffCountCHKdf['Site_ID'] == 2368]\n",
    "\n",
    "# Pivot the DataFrame to create separate columns for each unique Detector_ID\n",
    "pivoted_df = filtered_df.pivot(index=['Combined_DateTime', 'Site_ID'], columns='Detector_ID', values='Traffic_Volume')\n",
    "\n",
    "# Reset the index to make the combined DateTime and Site_ID columns regular columns\n",
    "pivoted_df = pivoted_df.reset_index()\n",
    "\n",
    "# Merge the pivoted DataFrame back into the original DataFrame based on the combined DateTime and Site_ID columns\n",
    "merged_df = filtered_df.merge(pivoted_df, on=['Combined_DateTime', 'Site_ID'], how='left')\n",
    "\n",
    "# Drop the 'Start_time' column\n",
    "merged_df.drop('Start_time', axis=1, inplace=True)\n",
    "\n",
    "# Print the updated DataFrame\n",
    "print(merged_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOR YOUR CHECK OLD CODES as I was building up the top cell for TraffCountCHKdfimport pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "TraffCountCHKdf = pd.read_csv('C:/Users/nsdsc0/OneDrive - WSP O365/DATA_ANALYTICS/Python/SCATS/WEST_20230308.VS.csv')\n",
    "TraffCountCHKdf\n",
    "\n",
    "\n",
    "# Convert 'Start_time' column to datetime format\n",
    "TraffCountCHKdf['Start_time'] = pd.to_datetime(TraffCountCHKdf['Start_time'])\n",
    "\n",
    "# Create a new column 'Combined_DateTime' by combining 'Date' and 'Start_time'\n",
    "TraffCountCHKdf['Combined_DateTime'] = TraffCountCHKdf['Date'].astype(str) + ' ' + TraffCountCHKdf['Start_time'].dt.time.astype(str)\n",
    "\n",
    "# Convert 'Combined_DateTime' column to datetime format\n",
    "TraffCountCHKdf['Combined_DateTime'] = pd.to_datetime(TraffCountCHKdf['Combined_DateTime'])\n",
    "\n",
    "# Round 'Combined_DateTime' column down to the nearest 5 minutes\n",
    "TraffCountCHKdf['Rounded_Combined_DateTime'] = TraffCountCHKdf['Combined_DateTime'].dt.floor('5min')\n",
    "\n",
    "#Filter the rows in the 'TraffCount3df' DataFrame where the 'Site_ID' column that has the value \"2368\"\n",
    "filtered_df = TraffCountCHKdf[TraffCountCHKdf['Site_ID'] == 2368]\n",
    "\n",
    "# Create a pivot table with 'Start_time' as index, 'Site_ID' as columns, and 'Detector_ID' as values\n",
    "pivot_table = pd.pivot_table(TraffCountCHKdf, values='Traffic_Volume', index='Start_time', columns='Site_ID', aggfunc='first')\n",
    "\n",
    "# Add the unique values of 'Detector_ID' as separate columns in 'TraffCount3df'\n",
    "for detector_id in TraffCountCHKdf['Detector_ID'].unique():\n",
    "    TraffCountCHKdf[detector_id] = pivot_table.loc[:, TraffCountCHKdf['Site_ID']].values\n",
    "\n",
    "# Print the updated DataFrame\n",
    "print(TraffCountCHKdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOR YOUR CHECK OLD CODES as I was building up the top cell for TraffCountCHKdf\n",
    "\n",
    "# Create a pivot table with 'Start_time' as index, 'Site_ID' as columns, and 'Detector_ID' as values\n",
    "pivot_table = pd.pivot_table(TraffCountCHKdf, values='Traffic_Volume', index='Start_time', columns='Site_ID', aggfunc='first')\n",
    "\n",
    "# Add the unique values of 'Detector_ID' as separate columns in 'TraffCount3df'\n",
    "for detector_id in TraffCount3df['Detector_ID'].unique():\n",
    "    TraffCount3df[detector_id] = pivot_table.loc[:, TraffCount3df['Site_ID']].values\n",
    "\n",
    "# Print the updated DataFrame\n",
    "print(TraffCount3df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#FOR YOUR CHECK OLD CODES as I was building up the top cell for TraffCountCHKdf\n",
    "# Get unique values of 'Detector_ID' column\n",
    "unique_detector_ids = TraffCount3df['Detector_ID'].unique()\n",
    "\n",
    "# Create new columns for each unique detector ID\n",
    "for detector_id in unique_detector_ids:\n",
    "    TraffCount3df[detector_id] = 0\n",
    "\n",
    "# Iterate over the rows and update the corresponding detector ID column with traffic volume\n",
    "for index, row in TraffCount3df.iterrows():\n",
    "    detector_id = row['Detector_ID']\n",
    "    TraffCount3df.loc[index, detector_id] = row['Traffic_Volume']\n",
    "\n",
    "# Print the updated DataFrame\n",
    "print(TraffCount3df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOR YOUR CHECK OLD CODES as I was building up the top cell for TraffCountCHKdf\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Combine the 'Date' and 'Start_time' columns\n",
    "TraffCount3df['Combined_DateTime'] = pd.to_datetime(TraffCount3df['Date'] + ' ' + TraffCount3df['Start_time'])\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(TraffCount3df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOR YOUR CHECK OLD CODES as I was building up the top cell for TraffCountCHKdf\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'TraffCount3df' is your DataFrame containing the 'Start_time' and 'Date' columns\n",
    "# Convert 'Start_time' column to datetime format\n",
    "TraffCount3df['Start_time'] = pd.to_datetime(TraffCount3df['Start_time'])\n",
    "\n",
    "# Create a new column 'Combined_DateTime' by combining 'Date' and 'Start_time'\n",
    "TraffCount3df['Combined_DateTime'] = TraffCount3df['Date'].astype(str) + ' ' + TraffCount3df['Start_time'].dt.time.astype(str)\n",
    "\n",
    "# Convert 'Combined_DateTime' column to datetime format\n",
    "TraffCount3df['Combined_DateTime'] = pd.to_datetime(TraffCount3df['Combined_DateTime'])\n",
    "\n",
    "# Round 'Combined_DateTime' column down to the nearest 5 minutes\n",
    "TraffCount3df['Rounded_Combined_DateTime'] = TraffCount3df['Combined_DateTime'].dt.floor('5min')\n",
    "\n",
    "# Print the updated DataFrame\n",
    "print(TraffCount3df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOR YOUR CHECK OLD CODES as I was building up the top cell for Signaldf\n",
    "\n",
    "#This code assumes that you have already imported the pandas library and that the 'Signaldf' DataFrame is already defined with the columns 'Cycle_ starttime' and 'Date'. \n",
    "#The code adds three new columns: 'Rounded_Time', 'Combined_DateTime', and 'Combined_DateTime' (renamed from 'Rounded_Combined_DateTime'). \n",
    "#It also moves the 'Combined_DateTime' column to be the 5th column in the DataFrame.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Round the 'Cycle_ starttime' column into 5 minutes intervals\n",
    "Signaldf['Rounded_Time'] = pd.to_datetime(Signaldf['Cycle_ starttime']).dt.floor('5min')\n",
    "\n",
    "# Create a new column by combining the 'Date' column and the rounded time\n",
    "Signaldf['Combined_DateTime'] = pd.to_datetime(Signaldf['Date']) + Signaldf['Rounded_Time'].dt.time\n",
    "\n",
    "# Round down the 'Combined_DateTime' column to the nearest 5 minutes\n",
    "Signaldf['Combined_DateTime'] = Signaldf['Combined_DateTime'].dt.floor('5min')\n",
    "\n",
    "# Rename the 'Rounded_Combined_DateTime' column to 'Combined_DateTime'\n",
    "Signaldf = Signaldf.rename(columns={'Rounded_Combined_DateTime': 'Combined_DateTime'})\n",
    "\n",
    "# Move the 'Combined_DateTime' column to be the 5th column\n",
    "cols = list(Signaldf.columns)\n",
    "cols.insert(4, cols.pop(cols.index('Combined_DateTime')))\n",
    "Signaldf = Signaldf[cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOR YOUR CHECK OLD CODES as I was building up the top cell for Signaldf\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "# Round 'Cycle_ starttime' to 5 minutes intervals\n",
    "Signaldf['Rounded_Cycle_starttime'] = Signaldf['Cycle_ starttime'].dt.floor('5min')\n",
    "\n",
    "# Combine 'Date' and rounded time\n",
    "Signaldf['Combined_DateTime'] = Signaldf['Date'] + timedelta(hours=Signaldf['Rounded_Cycle_starttime'].dt.hour,\n",
    "                                                             minutes=Signaldf['Rounded_Cycle_starttime'].dt.minute)\n",
    "\n",
    "# Round 'Combined_DateTime' down to the nearest 5 minutes\n",
    "Signaldf['Rounded_Combined_DateTime'] = Signaldf['Combined_DateTime'].dt.floor('5min')\n",
    "\n",
    "# Rename 'Rounded_Combined_DateTime' column to 'Combined_DateTime'\n",
    "Signaldf = Signaldf.rename(columns={'Rounded_Combined_DateTime': 'Combined_DateTime'})\n",
    "\n",
    "# Move 'Combined_DateTime' column to the 5th position\n",
    "cols = list(Signaldf.columns)\n",
    "cols.insert(4, cols.pop(cols.index('Combined_DateTime')))\n",
    "Signaldf = Signaldf[cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOR YOUR CHECK OLD CODES as I was building up the top cell for Signaldf\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Assuming 'Signaldf' is your DataFrame containing the 'Cycle_ starttime' and 'Date' columns\n",
    "# Convert 'Cycle_ starttime' column to datetime format\n",
    "Signaldf['Cycle_ starttime'] = pd.to_datetime(Signaldf['Cycle_ starttime'])\n",
    "\n",
    "# Round 'Cycle_ starttime' to the nearest 5 minutes using dt.floor\n",
    "Signaldf['Rounded_Time'] = Signaldf['Cycle_ starttime'].dt.floor('5min')\n",
    "\n",
    "# Combine 'Date' and 'Rounded_Time' into a new column\n",
    "Signaldf['Combined_DateTime'] = Signaldf['Date'].astype(str) + ' ' + Signaldf['Rounded_Time'].dt.time.astype(str)\n",
    "\n",
    "# Round 'Combined_DateTime' column down to the nearest 5 minutes\n",
    "Signaldf['Rounded_Combined_DateTime'] = pd.to_datetime(Signaldf['Combined_DateTime']).dt.floor('5min')\n",
    "\n",
    "# Print the updated DataFrame\n",
    "print(Signaldf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#FOR YOUR CHECK OLD CODES as I was building up the top cell for Signaldf\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Assuming 'Signaldf' is your DataFrame containing the 'Cycle_ starttime' and 'Date' columns\n",
    "# Convert 'Cycle_ starttime' column to datetime format\n",
    "Signaldf['Cycle_ starttime'] = pd.to_datetime(Signaldf['Cycle_ starttime'])\n",
    "\n",
    "# Round 'Cycle_ starttime' to the nearest 5 minutes using dt.floor\n",
    "Signaldf['Rounded_Time'] = Signaldf['Cycle_ starttime'].dt.floor('5min')\n",
    "\n",
    "# Combine 'Date' and 'Rounded_Time' into a new column\n",
    "Signaldf['Combined_DateTime'] = Signaldf['Date'].astype(str) + ' ' + Signaldf['Rounded_Time'].dt.time.astype(str)\n",
    "\n",
    "# Round 'Combined_DateTime' column down to the nearest 5 minutes\n",
    "Signaldf['Rounded_Combined_DateTime'] = pd.to_datetime(Signaldf['Combined_DateTime']).dt.floor('5min')\n",
    "\n",
    "# Print the updated DataFrame\n",
    "print(Signaldf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#FOR YOUR CHECK OLD CODES as I was building up the top cell for Signaldf\n",
    "import pandas as pd\n",
    "\n",
    "# Round the 'Cycle_starttime' column to the nearest 5-minute interval\n",
    "Signaldf['Rounded_Time'] = Signaldf['Cycle_starttime'].dt.floor('5T')\n",
    "\n",
    "# Combine the 'Date' column and the rounded time\n",
    "Signaldf['Combined_DateTime'] = Signaldf['Date'].astype(str) + ' ' + Signaldf['Rounded_Time'].dt.time.astype(str)\n",
    "\n",
    "# Print the updated dataframe\n",
    "print(Signaldf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#FOR YOUR CHECK OLD CODES as I was building up the top cell for Signaldf\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'Signaldf' is your DataFrame and 'Cycle_ starttime' is the column of interest\n",
    "\n",
    "# Convert 'Cycle_ starttime' to datetime type if it's not already\n",
    "Signaldf['Cycle_ starttime'] = pd.to_datetime(Signaldf['Cycle_ starttime'])\n",
    "\n",
    "# Round the 'Cycle_ starttime' to the nearest 5-minute interval. THIS IS THE OLD CODE given by chatGPT '5Min'\n",
    "Signaldf['Rounded starttime'] = Signaldf['Cycle_ starttime'].dt.floor('5Min')\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(Signaldf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOR YOUR CHECK OLD CODES as I was building up the top cell for Signaldf\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Assuming 'Signaldf' is your DataFrame with the 'Cycle_ starttime' column\n",
    "# Create a sample DataFrame for demonstration\n",
    "data = {\n",
    "    'Cycle_starttime': ['00:01:00', '00:02:00', '00:03:00', '00:04:00', '00:06:00', '00:07:00', '00:09:00']\n",
    "}\n",
    "Signaldf = pd.DataFrame(data)\n",
    "\n",
    "# Convert 'Cycle_starttime' column to datetime format\n",
    "Signaldf['Cycle_starttime'] = pd.to_datetime(Signaldf['Cycle_starttime'])\n",
    "\n",
    "# Round the 'Cycle_starttime' column to 5-minute intervals\n",
    "Signaldf['Rounded_starttime'] = Signaldf['Cycle_starttime'].apply(lambda x: (x - timedelta(minutes=x.minute % 5)) + timedelta(minutes=5))\n",
    "\n",
    "# Convert 'Rounded_starttime' column back to string format\n",
    "Signaldf['Rounded_starttime'] = Signaldf['Rounded_starttime'].dt.strftime('%H:%M:%S')\n",
    "\n",
    "print(Signaldf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#FOR YOUR CHECK OLD CODES as I was building up the top cell for Signaldf\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Assuming 'Signaldf' is your DataFrame with the 'Cycle_starttime' and 'Date' columns\n",
    "# Create a sample DataFrame for demonstration\n",
    "data = {\n",
    "    'Cycle_starttime': ['00:01:00', '00:02:00', '00:03:00', '00:04:00', '00:06:00', '00:07:00', '00:09:00'],\n",
    "    'Date': ['2023-05-31', '2023-05-31', '2023-05-31', '2023-05-31', '2023-05-31', '2023-05-31', '2023-05-31']\n",
    "}\n",
    "Signaldf = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "\n",
    "# Round the 'Cycle_starttime' column to 5-minute intervals\n",
    "Signaldf['Rounded_starttime'] = Signaldf.apply(lambda row: (row['Cycle_starttime'] - timedelta(minutes=row['Cycle_starttime'].minute % 5)) + timedelta(minutes=5), axis=1)\n",
    "\n",
    "# Combine 'Date' and 'Rounded_starttime' columns and convert to string format\n",
    "Signaldf['Rounded_starttime'] = Signaldf['Rounded_starttime'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "print(Signaldf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TraffCount3df = pd.read_csv('C:/Users/nsdsc0/OneDrive - WSP O365/DATA_ANALYTICS/Python/SCATS/WEST_20230308.VS.csv')\n",
    "Traffic_column = TraffCount3df.columns.tolist()\n",
    "# Print the column names\n",
    "for column in Traffic_column:\n",
    " print(column)\n",
    " print(TraffCount3df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = TraffCount3df[TraffCount3df['Site_ID'] == '2368']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Date' column to datetime\n",
    "TraffCount2df['Date'] = pd.to_datetime(TraffCount2df['Date'])\n",
    "\n",
    "# Convert 'Start_time' column to datetime\n",
    "TraffCount2df['Start_time'] = pd.to_datetime(TraffCount2df['Start_time'], format='%H:%M:%S')\n",
    "\n",
    "# Modify the format of 'Start_time' column\n",
    "TraffCount2df['Start_time'] = TraffCount2df['Start_time'].dt.strftime('%H:%M:%S')\n",
    "\n",
    "# Create a new column combining 'Date' and 'Start_time'\n",
    "TraffCount2df['Combined_datetime'] = TraffCount2df['Date'].astype(str) + ' ' + TraffCount2df['Start_time']\n",
    "\n",
    "# Print the updated TraffCount2df\n",
    "print(TraffCount2df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the corresponding Detector_Ids\n",
    "filt_detec_ids = TraffCount2df.loc[TraffCount2df['Site_ID'] == 2368, 'Detector_ID']\n",
    "\n",
    "# Create new columns with extracted Detector_Ids\n",
    "for Detector_Id in filt_detec_ids:\n",
    "    TraffCount2df[Detector_Id] = None  # Initialize the new column with None\n",
    "\n",
    "# Print the updated TraffCount2df\n",
    "# print(TraffCount2df)\n",
    "\n",
    "# Define a custom function to fill the Detector Id columns\n",
    "def fill_traffic_volume(row):\n",
    "    site_id = row['Site_ID']\n",
    "    start_time = row['Start_time']\n",
    "    traffic_volume = row['Traffic_Volume']\n",
    "\n",
    "    # Iterate over the filtered Detector Ids for Site ID 2368\n",
    "    for detector_id in filt_detec_ids:\n",
    "        if (row['Detector_ID'] == detector_id) and (row['Site_ID'] == site_id):\n",
    "            return traffic_volume\n",
    "\n",
    "    return None  # Return None if no matching Detector Id is found\n",
    "\n",
    "# Apply the custom function to fill the Detector Id columns\n",
    "for detector_id in filt_detec_ids:\n",
    "    TraffCount2df[detector_id] = TraffCount2df.apply(fill_traffic_volume, axis=1)\n",
    "\n",
    "# Print the updated TraffCount2df\n",
    "print(TraffCount2df)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
